
import gradio as gr
import torch
import torchvision.transforms as transforms
from torchvision import models
from PIL import Image
import os
import torch.nn.functional as F
import json
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

matplotlib.use('Agg')  # GUI olmadan backend kullan

# Cihaz seÃ§imi
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"KullanÄ±lan cihaz: {device}")

# Model bilgilerini yÃ¼kleme
try:
    if os.path.exists("model_info.json"):
        with open("model_info.json", "r") as f:
            model_info = json.load(f)
        model_path = model_info["model_path"]
        classes = model_info["class_names"]
        img_size = model_info["img_size"]
        print(f"Model bilgileri yÃ¼klendi: {model_path}")
        print(f"SÄ±nÄ±flar: {classes}")
    else:
        # Model bilgileri yoksa varsayÄ±lan deÄŸerleri kullan
        model_path = "best_model.pth"
        classes = sorted(os.listdir("./raw-img"))
        img_size = 224
        print(f"Model bilgi dosyasÄ± bulunamadÄ±, varsayÄ±lan deÄŸerler kullanÄ±lÄ±yor.")
        print(f"VarsayÄ±lan model yolu: {model_path}")
        print(f"Tespit edilen sÄ±nÄ±flar: {classes}")
except Exception as e:
    print(f"Model bilgileri yÃ¼klenirken hata oluÅŸtu: {e}")
    model_path = "best_model.pth"
    classes = ["SÄ±nÄ±f bilgisi yÃ¼klenemedi"]
    img_size = 224


# Model yÃ¼kleme
def load_model():
    try:
        print(f"Model yÃ¼kleniyor: {model_path}")
        if os.path.exists(model_path):
            # Checkpoint formatÄ±nda kaydedilmiÅŸ modeli yÃ¼kleme
            checkpoint = torch.load(model_path, map_location=device)

            # EÄŸer checkpoint bir sÃ¶zlÃ¼k ise
            if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
                model_state = checkpoint["model_state_dict"]
                if "classes" in checkpoint:
                    global classes
                    classes = checkpoint["classes"]
                    print(f"Checkpoint'ten sÄ±nÄ±f isimleri gÃ¼ncellendi: {classes}")
            else:
                model_state = checkpoint

            # Model mimarisini belirle (ResNet18 varsayÄ±lan)
            model = models.resnet18(pretrained=False)
            model.fc = torch.nn.Linear(model.fc.in_features, len(classes))

            # Model durumunu yÃ¼kle
            model.load_state_dict(model_state)
            model = model.to(device)
            model.eval()
            print("Model baÅŸarÄ±yla yÃ¼klendi.")
            return model
        else:
            print(f"Model dosyasÄ± bulunamadÄ±: {model_path}")
            return None
    except Exception as e:
        print(f"Model yÃ¼klenirken hata oluÅŸtu: {e}")
        import traceback
        traceback.print_exc()
        return None


# Modeli yÃ¼kle
model = load_model()

# GÃ¶rsel dÃ¶nÃ¼ÅŸÃ¼mlerini tanÄ±mla
transform = transforms.Compose([
    transforms.Resize((img_size, img_size)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])


# SÄ±nÄ±flandÄ±rma sonucunu gÃ¶rselleÅŸtirme
def visualize_prediction(probs, class_names):
    top_k = 5
    if len(class_names) < top_k:
        top_k = len(class_names)

    # En yÃ¼ksek olasÄ±lÄ±klÄ± sÄ±nÄ±flarÄ± al
    top_probs, top_indices = torch.topk(probs, k=top_k)

    # Numpy dizisine dÃ¶nÃ¼ÅŸtÃ¼r
    top_probs = top_probs.cpu().numpy()
    top_indices = top_indices.cpu().numpy()

    # SÄ±nÄ±f isimlerini al
    top_classes = [class_names[i] for i in top_indices]

    # GrafiÄŸi oluÅŸtur
    fig, ax = plt.subplots(figsize=(10, 6))

    # Yatay bar plot
    y_pos = np.arange(len(top_classes))
    ax.barh(y_pos, top_probs * 100, color='skyblue')
    ax.set_yticks(y_pos)
    ax.set_yticklabels(top_classes)
    ax.invert_yaxis()  # Etiketleri yukarÄ±dan aÅŸaÄŸÄ±ya sÄ±rala
    ax.set_xlabel('OlasÄ±lÄ±k (%)')
    ax.set_title('En OlasÄ± SÄ±nÄ±flar')

    # DeÄŸerleri bar'larÄ±n yanÄ±nda gÃ¶ster
    for i, v in enumerate(top_probs):
        ax.text(v * 100 + 1, i, f"{v * 100:.1f}%", va='center')

    plt.tight_layout()

    # GeÃ§ici dosya olarak kaydet
    import tempfile
    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
        plt.savefig(tmp.name)
        plt.close()
        return tmp.name


# Tahmin fonksiyonu
def predict(image):
    if model is None:
        return None, "Model yÃ¼klenemedi. LÃ¼tfen sistem yÃ¶neticinizle iletiÅŸime geÃ§in.", None

    if image is None:
        return None, "LÃ¼tfen bir gÃ¶rsel yÃ¼kleyin.", None

    try:
        # GÃ¶rÃ¼ntÃ¼ Ã¶n iÅŸleme
        img_processed = transform(image).unsqueeze(0).to(device)

        # Tahmin yapma
        with torch.no_grad():
            outputs = model(img_processed)
            probs = F.softmax(outputs, dim=1)[0]

        # En yÃ¼ksek olasÄ±lÄ±ÄŸa sahip sÄ±nÄ±fÄ± bul
        confidence, predicted_idx = torch.max(probs, 0)
        predicted_class = classes[predicted_idx.item()]
        confidence_score = confidence.item() * 100

        # Sonucu dÃ¼zenle
        result_text = f"Tahmin: {predicted_class}\nGÃ¼ven Skoru: {confidence_score:.2f}%"

        # SÄ±nÄ±flandÄ±rma sonucunu gÃ¶rselleÅŸtir
        viz_path = visualize_prediction(probs, classes)

        return image, result_text, viz_path
    except Exception as e:
        import traceback
        error_msg = f"Tahmin sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}"
        traceback.print_exc()
        return image, error_msg, None


# Ã–rnek gÃ¶rÃ¼ntÃ¼ seÃ§me fonksiyonu
def get_example_images():
    examples = []
    try:
        example_dir = "./raw-img"
        if not os.path.exists(example_dir):
            return []

        for class_name in classes:
            class_path = os.path.join(example_dir, class_name)
            if os.path.isdir(class_path):
                files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
                if files:
                    # Her sÄ±nÄ±ftan bir Ã¶rnek seÃ§
                    examples.append(os.path.join(class_path, files[0]))
    except Exception as e:
        print(f"Ã–rnek gÃ¶rseller yÃ¼klenirken hata oluÅŸtu: {e}")

    return examples


# Gradio arayÃ¼zÃ¼
def create_interface():
    with gr.Blocks(title="GÃ¶rÃ¼ntÃ¼ SÄ±nÄ±flandÄ±rÄ±cÄ±") as interface:
        gr.Markdown("# ğŸ” GÃ¶rÃ¼ntÃ¼ SÄ±nÄ±flandÄ±rÄ±cÄ±")
        gr.Markdown("Bir gÃ¶rÃ¼ntÃ¼ yÃ¼kleyin ve modelin hangi sÄ±nÄ±fa ait olduÄŸunu tahmin etmesini izleyin.")

        with gr.Row():
            with gr.Column(scale=1):
                input_image = gr.Image(type="pil", label="GÃ¶rÃ¼ntÃ¼ YÃ¼kle")
                predict_btn = gr.Button("Tahmin Et", variant="primary")

            with gr.Column(scale=1):
                output_image = gr.Image(type="pil", label="YÃ¼klenen GÃ¶rÃ¼ntÃ¼")
                output_text = gr.Textbox(label="Tahmin Sonucu")
                output_plot = gr.Image(label="SÄ±nÄ±f OlasÄ±lÄ±klarÄ±")

        predict_btn.click(fn=predict, inputs=input_image, outputs=[output_image, output_text, output_plot])

        # Ã–rnek gÃ¶rselleri ekle
        examples = get_example_images()
        if examples:
            gr.Examples(
                examples=examples,
                inputs=input_image,
                outputs=[output_image, output_text, output_plot],
                fn=predict,
                cache_examples=True
            )

        gr.Markdown("## NasÄ±l KullanÄ±lÄ±r")
        gr.Markdown("""
        1. 'GÃ¶rÃ¼ntÃ¼ YÃ¼kle' alanÄ±na bir gÃ¶rÃ¼ntÃ¼ yÃ¼kleyin veya Ã¶rnek gÃ¶rsellerden birini seÃ§in
        2. 'Tahmin Et' butonuna tÄ±klayÄ±n
        3. Model tahminini ve gÃ¼ven skorunu saÄŸ tarafta gÃ¶receksiniz
        """)

        if classes != ["SÄ±nÄ±f bilgisi yÃ¼klenemedi"]:
            class_info = ", ".join(classes)
            gr.Markdown(f"**TanÄ±mlÄ± SÄ±nÄ±flar:** {class_info}")

    return interface


# Ana fonksiyon
def main():
    interface = create_interface()
    interface.launch(share=False, server_name="127.0.0.1")


if __name__ == "__main__":
    main()